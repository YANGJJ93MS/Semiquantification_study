{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path1 = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/499_500_501_area.csv'\n",
    "# path1 = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/NEW_acquisition_20241018/Area_1_2024_10_21_14_51_42.csv'\n",
    "path2 = 'D:/UCSF_postdoc_topic/ESI_/ENTACT/first_ENTACT_mixtures_mzlist.csv'\n",
    "path4 = 'D:/UCSF_postdoc_topic/ESI_/EPA_pesticide_DTSC_phthalates/Pesticide_phthlates_mzlist.csv'\n",
    "path5 = 'D:/UCSF_postdoc_topic/ESI_/EPA_pesticide_DTSC_phthalates/IS_for_eachbatch_mzlist.csv'\n",
    "# output_fd = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_502_504_506/peak_area_with_blank_filter/'\n",
    "RTpath6 = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/ENTACTmix_RT_manualcheck.csv'\n",
    "RTpath7 = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/Othermixture_RT_manualcheck.csv'\n",
    "output_fd = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/'\n",
    "peakareadata = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/SQ_oct_ENTACT_EPA_PHTH_pos/Area_1_2024_10_31_14_05_32_ENACT_pesticide_phth_pos.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      RT  AlignID  Average_MZ  pos_EPA499_100X  pos_EPA500_100X  \\\n",
      "0  1.566        0   100.07566          5941903          4890390   \n",
      "1  0.750        1   100.07571          8426027          4117234   \n",
      "2  0.081        2   100.07571          3095804          5367883   \n",
      "3  1.308        3   100.07574         14109951          3013204   \n",
      "4  3.261        4   100.07581          3205843         11335106   \n",
      "\n",
      "   pos_EPA501_100X  pos_EPA502_100X  pos_EPA504_100X  pos_EPA506_100X  \\\n",
      "0          2719208          5288036          3844434          3751659   \n",
      "1          5858812         11279772         12348388         11763927   \n",
      "2          3632987          4202877          3622470          6990118   \n",
      "3         15122946          2850842         14167199         13391818   \n",
      "4          3284039          6939958         13422485         19663452   \n",
      "\n",
      "   pos_EPA507_100X  pos_EPA508_100X  \n",
      "0          6022275          6170355  \n",
      "1         13112304         13541033  \n",
      "2          3198011          5714371  \n",
      "3          2969711         16035560  \n",
      "4          5904879          9842527  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Data\n",
    "# Load the raw LC-MS data file\n",
    "#path1 = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/499_500_501_area.csv'\n",
    "# path1 = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_502_504_506/peak_area_with_blank_filter/502_504_506_area.csv'\n",
    "path1 = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/NEW_acquisition_20241018/Area_1_2024_10_21_14_51_42.csv'\n",
    "feature_data = pd.read_csv(path1)\n",
    "\n",
    "# Step 2: Extract Required Columns\n",
    "# Extract 'Average Rt(min)', 'Alignment ID', 'Average Mz', and sample columns\n",
    "columns_to_keep = ['RT', 'AlignID', 'Average_MZ']\n",
    "# sample_columns = [col for col in feature_data.columns if col.startswith(('pos_EPA502', 'pos_EPA504', 'pos_EPA506',\n",
    "#                                                                          'pos_EPA499', 'pos_EPA500', 'pos_EPA501',\n",
    "#                                                                          'pos_EPA507', 'pos_EPA508'))]\n",
    "\n",
    "\n",
    "#For new acquisition 'pos_EPA499_100X'\n",
    "columns_to_keep.extend(sample_columns)\n",
    "filtered_data = feature_data.rename(columns={'Average Rt(min)': 'RT', 'Alignment ID': 'AlignID', 'Average Mz': 'Average_MZ'})[columns_to_keep]\n",
    "\n",
    "# Step 3: Print the Head of the DataFrame\n",
    "print(filtered_data.head())\n",
    "# filtered_data.to_csv('D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_502_504_506/peak_area_with_blank_filter/502_504_506_area_filtered.csv', index=False)\n",
    "filtered_data.to_csv('D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/499_500_501_502_504_506_507_508_area_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique groups found: ['pos_EPA499' 'pos_EPA500' 'pos_EPA501' 'pos_EPA502' 'pos_EPA504'\n",
      " 'pos_EPA506' 'pos_EPA507' 'pos_EPA508']\n",
      "Processing group: pos_EPA499, Columns extracted: ['Average_MZ', 'RT', 'pos_EPA499_100X']\n",
      "Filtered data saved for group pos_EPA499: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA499.csv\n",
      "Processing group: pos_EPA500, Columns extracted: ['Average_MZ', 'RT', 'pos_EPA500_100X']\n",
      "Filtered data saved for group pos_EPA500: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA500.csv\n",
      "Processing group: pos_EPA501, Columns extracted: ['Average_MZ', 'RT', 'pos_EPA501_100X']\n",
      "Filtered data saved for group pos_EPA501: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA501.csv\n",
      "Processing group: pos_EPA502, Columns extracted: ['Average_MZ', 'RT', 'pos_EPA502_100X']\n",
      "Filtered data saved for group pos_EPA502: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA502.csv\n",
      "Processing group: pos_EPA504, Columns extracted: ['Average_MZ', 'RT', 'pos_EPA504_100X']\n",
      "Filtered data saved for group pos_EPA504: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA504.csv\n",
      "Processing group: pos_EPA506, Columns extracted: ['Average_MZ', 'RT', 'pos_EPA506_100X']\n",
      "Filtered data saved for group pos_EPA506: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA506.csv\n",
      "Processing group: pos_EPA507, Columns extracted: ['Average_MZ', 'RT', 'pos_EPA507_100X']\n",
      "Filtered data saved for group pos_EPA507: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA507.csv\n",
      "Processing group: pos_EPA508, Columns extracted: ['Average_MZ', 'RT', 'pos_EPA508_100X']\n",
      "Filtered data saved for group pos_EPA508: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA508.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the Cleaned Data\n",
    "# Load the cleaned LC-MS data file\n",
    "# path1 = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_502_504_506/peak_area_with_blank_filter/502_504_506_area_filtered.csv'\n",
    "# filtered_data = pd.read_csv(path1)\n",
    "\n",
    "# Step 2: Separate Data by Groups\n",
    "# Extract the data for each group separately\n",
    "# unique_groups = filtered_data.columns.str.extract(r'(E\\d{3})')[0].dropna().unique()\n",
    "unique_groups = filtered_data.columns.str.extract(r'(pos_EPA\\d{3})')[0].dropna().unique()\n",
    "print(f\"Unique groups found: {unique_groups}\")\n",
    "\n",
    "# Step 3: Iterate over each group\n",
    "for group in unique_groups:\n",
    "    # Extract group-specific data, keep 'Average_MZ' and 'RT' columns\n",
    "    group_data = filtered_data.filter(like=group)\n",
    "    group_data = pd.concat([filtered_data[['Average_MZ', 'RT']], group_data], axis=1)\n",
    "    print(f\"Processing group: {group}, Columns extracted: {group_data.columns.tolist()}\")\n",
    "\n",
    "    # Step 4: Calculate average intensity and CV% for each treatment and add to the data\n",
    "    # for treatment in [\"100x\", \"200x\", \"1000x\", \"5000x\", \"25000x\"]:\n",
    "    for treatment in [\"100X\"]:\n",
    "        treatment_cols = [col for col in group_data.columns if f\"_{treatment}_\" in col]\n",
    "        if len(treatment_cols) == 0:\n",
    "            group_data[f\"avg_intensity_{treatment}\"] = np.nan\n",
    "            group_data[f\"cv_percent_{treatment}\"] = np.nan\n",
    "            continue\n",
    "        treatment_data = group_data[treatment_cols]\n",
    "        cv_percent = treatment_data.std(axis=1) / treatment_data.mean(axis=1) * 100\n",
    "        avg_intensity = treatment_data.mean(axis=1)\n",
    "        group_data[f\"avg_intensity_{treatment}\"] = np.where(cv_percent <= 30, avg_intensity, np.nan)\n",
    "        group_data[f\"cv_percent_{treatment}\"] = cv_percent\n",
    "\n",
    "    # Step 5: Export Group Data\n",
    "    # output_folder = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_502_504_506/peak_area_with_blank_filter/'\n",
    "    output_folder = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    filtered_data_path = os.path.join(output_folder, f\"filtered_feature_table_{group}.csv\")\n",
    "    group_data.to_csv(filtered_data_path, index=False)\n",
    "    print(f\"Filtered data saved for group {group}: {filtered_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Target m/z Data\n",
    "# Load the target m/z data file\n",
    "path = 'D:/UCSF_postdoc_topic/ESI_/ENTACT/first_ENTACT_mixtures_mzlist.csv'\n",
    "target_mz_data = pd.read_csv(path)\n",
    "\n",
    "# Step 2: Divide Concentration Columns by 1000\n",
    "# Assuming the concentration columns are named 'conc1', 'conc2', etc.\n",
    "concentration_columns = [col for col in target_mz_data.columns if 'conc' in col]\n",
    "target_mz_data[concentration_columns] = target_mz_data[concentration_columns] / 1000\n",
    "\n",
    "# Step 3: Save the Processed Data\n",
    "# Save the preprocessed target m/z data to a new CSV file\n",
    "# output_path = 'D:/UCSF_postdoc_topic/ESI_/ENTACT/processed_target_mzlist.csv'\n",
    "# target_mz_data.to_csv(output_path, index=False)\n",
    "# print(f\"Preprocessed data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>DTXSID</th>\n",
       "      <th>compound_id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>MOLECULAR_FORMULA</th>\n",
       "      <th>m/z</th>\n",
       "      <th>conc1</th>\n",
       "      <th>conc2</th>\n",
       "      <th>conc3</th>\n",
       "      <th>conc4</th>\n",
       "      <th>conc5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E499</td>\n",
       "      <td>DTXSID5047328</td>\n",
       "      <td>MK-274</td>\n",
       "      <td>O.[K+].NC(=O)C1=NC(=N[N-]1)C1=CC=CC(=C1)C1=CC(...</td>\n",
       "      <td>C18H13F6KN4O3</td>\n",
       "      <td>486.052891</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E499</td>\n",
       "      <td>DTXSID5027198</td>\n",
       "      <td>N-Methylphthalimide</td>\n",
       "      <td>CN1C(=O)C2=C(C=CC=C2)C1=O</td>\n",
       "      <td>C9H7NO2</td>\n",
       "      <td>161.047678</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E499</td>\n",
       "      <td>DTXSID9032113</td>\n",
       "      <td>Tebuconazole</td>\n",
       "      <td>CC(C)(C)C(O)(CCC1=CC=C(Cl)C=C1)CN1C=NC=N1</td>\n",
       "      <td>C16H22ClN3O</td>\n",
       "      <td>307.145140</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E499</td>\n",
       "      <td>DTXSID6024466</td>\n",
       "      <td>4-Aminobenzoic acid</td>\n",
       "      <td>NC1=CC=C(C=C1)C(O)=O</td>\n",
       "      <td>C7H7NO2</td>\n",
       "      <td>137.047678</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E499</td>\n",
       "      <td>DTXSID4040002</td>\n",
       "      <td>Monobutyl phthalate</td>\n",
       "      <td>CCCCOC(=O)C1=C(C=CC=C1)C(O)=O</td>\n",
       "      <td>C12H14O4</td>\n",
       "      <td>222.089209</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group_id         DTXSID          compound_id  \\\n",
       "0     E499  DTXSID5047328               MK-274   \n",
       "1     E499  DTXSID5027198  N-Methylphthalimide   \n",
       "2     E499  DTXSID9032113         Tebuconazole   \n",
       "3     E499  DTXSID6024466  4-Aminobenzoic acid   \n",
       "4     E499  DTXSID4040002  Monobutyl phthalate   \n",
       "\n",
       "                                              SMILES MOLECULAR_FORMULA  \\\n",
       "0  O.[K+].NC(=O)C1=NC(=N[N-]1)C1=CC=CC(=C1)C1=CC(...     C18H13F6KN4O3   \n",
       "1                          CN1C(=O)C2=C(C=CC=C2)C1=O           C9H7NO2   \n",
       "2          CC(C)(C)C(O)(CCC1=CC=C(Cl)C=C1)CN1C=NC=N1       C16H22ClN3O   \n",
       "3                               NC1=CC=C(C=C1)C(O)=O           C7H7NO2   \n",
       "4                      CCCCOC(=O)C1=C(C=CC=C1)C(O)=O          C12H14O4   \n",
       "\n",
       "          m/z  conc1  conc2  conc3  conc4   conc5  \n",
       "0  486.052891    0.2    0.1   0.02  0.004  0.0008  \n",
       "1  161.047678    0.2    0.1   0.02  0.004  0.0008  \n",
       "2  307.145140    0.2    0.1   0.02  0.004  0.0008  \n",
       "3  137.047678    0.2    0.1   0.02  0.004  0.0008  \n",
       "4  222.089209    0.2    0.1   0.02  0.004  0.0008  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mz_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined targeted m/z analysis results saved: D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/combined_targeted_mz_results_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load Processed Data and Targeted m/z List\n",
    "# Load the processed LC-MS data file\n",
    "# change the batch ID to each results\n",
    "# processed_data_path = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_502_504_506/peak_area_with_blank_filter/filtered_feature_table_E506.csv'\n",
    "processed_data_path = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/filtered_feature_table_pos_EPA506.csv'\n",
    "processed_data = pd.read_csv(processed_data_path)\n",
    "\n",
    "# Load the targeted m/z list\n",
    "targeted_mz_list_path = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_502_504_506/peak_area_with_blank_filter/506_batch_target_mzlist.csv'\n",
    "targeted_mz_list = pd.read_csv(targeted_mz_list_path)\n",
    "\n",
    "# Step 2: Initialize an Empty DataFrame to Store Results\n",
    "combined_results = pd.DataFrame()\n",
    "\n",
    "# Step 3: Iterate Over Each Targeted m/z\n",
    "for idx, row in targeted_mz_list.iterrows():\n",
    "    target_mz = row['m/z']\n",
    "    concentrations = [row['conc1'], row['conc2'], row['conc3'], row['conc4'], row['conc5']]\n",
    "\n",
    "    # Step 4: Find m/z Matches Within 10 ppm\n",
    "    ppm_tolerance = 50\n",
    "    matching_rows = processed_data.copy()\n",
    "    matching_rows['ppm'] = abs((matching_rows['Average_MZ'] - target_mz) / target_mz) * 1e6\n",
    "    matching_rows = matching_rows[matching_rows['ppm'] <= ppm_tolerance]\n",
    "\n",
    "    # Step 5: Add Metadata from Targeted m/z List to Matching Rows\n",
    "    for col in ['group_id', 'DTXSID', 'compound_id', 'SMILES', 'MOLECULAR_FORMULA','m/z']:\n",
    "        matching_rows[col] = row[col]\n",
    "\n",
    "    # Step 6: Calculate LogRF for Each Treatment/Concentration Pair with CV Filtering\n",
    "    logRF_columns = []\n",
    "    for i, (treatment, conc) in enumerate(zip([\"100x\", \"200x\", \"1000x\", \"5000x\", \"25000x\"], concentrations), start=1):\n",
    "        avg_intensity_col = f\"avg_intensity_{treatment}\"\n",
    "        cv_col = f\"cv_percent_{treatment}\"\n",
    "        logRF_col = f'LogRF_{i}'\n",
    "        if avg_intensity_col in matching_rows.columns and cv_col in matching_rows.columns and conc != 0:\n",
    "            matching_rows[logRF_col] = np.where(matching_rows[cv_col] <= 30, np.log10(matching_rows[avg_intensity_col] / conc), np.nan)\n",
    "        else:\n",
    "            matching_rows[logRF_col] = np.nan\n",
    "        logRF_columns.append(logRF_col)\n",
    "\n",
    "    # Step 7: Calculate Average LogRF if More Than 3 Values are Available\n",
    "    matching_rows['average_LogRF'] = matching_rows[logRF_columns].apply(lambda x: x[x.notna()].mean() if x.count() >= 3 else np.nan, axis=1)\n",
    "\n",
    "    # Step 8: Combine Results\n",
    "    combined_results = pd.concat([combined_results, matching_rows], ignore_index=True)\n",
    "\n",
    "# Step 9: Filter Combined Results to Keep Only Rows with Valid average_LogRF and Lowest ppm\n",
    "filtered_results = combined_results[combined_results['average_LogRF'].notna()]\n",
    "filtered_results = filtered_results.sort_values(by=['m/z', 'ppm']).drop_duplicates(subset='m/z', keep='first')\n",
    "\n",
    "# Step 10: Export Combined Results\n",
    "# output_folder = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/targeted_res/'\n",
    "output_folder = 'D:/UCSF_postdoc_topic/ESI_/new_data_acquisition/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "group_id = filtered_results['group_id'].iloc[0] if not filtered_results.empty else 'combined'\n",
    "combined_results_path = os.path.join(output_folder, f\"combined_targeted_mz_results_{group_id}.csv\")\n",
    "filtered_results.to_csv(combined_results_path, index=False)\n",
    "print(f\"Combined targeted m/z analysis results saved: {combined_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (166, 112)\n",
      "Number of duplicated 'compound_id' values: 1\n",
      "Duplicated rows:\n",
      "    Average_MZ     RT  E499_1000x_r1  E499_1000x_r2  E499_1000x_r3  \\\n",
      "5    253.14082  7.006        17551.0        17067.0        20831.0   \n",
      "39   253.14067  7.005            NaN            NaN            NaN   \n",
      "\n",
      "    E499_100x_r1  E499_100x_r2  E499_100x_r3  E499_200x_r1  E499_200x_r2  ...  \\\n",
      "5        16054.0       15332.0       14708.0       18666.0       16463.0  ...   \n",
      "39           NaN           NaN           NaN           NaN           NaN  ...   \n",
      "\n",
      "    E506_100x_r3  E506_200x_r1  E506_200x_r2  E506_200x_r3  E506_25000x_r1  \\\n",
      "5            NaN           NaN           NaN           NaN             NaN   \n",
      "39           NaN           NaN           NaN           NaN             NaN   \n",
      "\n",
      "    E506_25000x_r2  E506_25000x_r3  E506_5000x_r1_r1  E506_5000x_r1_r2  \\\n",
      "5              NaN             NaN               NaN               NaN   \n",
      "39             NaN             NaN               NaN               NaN   \n",
      "\n",
      "    E506_5000x_r1_r3  \n",
      "5                NaN  \n",
      "39               NaN  \n",
      "\n",
      "[2 rows x 112 columns]\n"
     ]
    }
   ],
   "source": [
    "# open folder and import all .csv file in the designated folder and combined all dataframe\n",
    "#check and count the duplicated value in compound_id column\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load All CSV Files from the Folder\n",
    "folder_path = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/targeted_res/'\n",
    "all_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Step 2: Import and Combine All DataFrames\n",
    "combined_df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "\n",
    "# Step 3: Check and Count Duplicated Values in 'compound_id' Column\n",
    "duplicated_count = combined_df['compound_id'].duplicated().sum()\n",
    "print(f\"Number of duplicated 'compound_id' values: {duplicated_count}\")\n",
    "\n",
    "# Step 4: Print Duplicated Rows\n",
    "duplicated_rows = combined_df[combined_df.duplicated(subset='compound_id', keep=False)]\n",
    "print(\"Duplicated rows:\")\n",
    "print(duplicated_rows)\n",
    "\n",
    "combined_df.to_csv('D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/targeted_res/combined_logRF.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined targeted m/z analysis results saved: D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/targeted_res/combined_targeted_mz_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load Processed Data and Targeted m/z List\n",
    "# Load the processed LC-MS data file\n",
    "processed_data_path = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/filtered_feature_table_E499.csv'\n",
    "processed_data = pd.read_csv(processed_data_path)\n",
    "\n",
    "# Load the targeted m/z list\n",
    "targeted_mz_list_path = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/499_batch_target_mzlist.csv'\n",
    "targeted_mz_list = pd.read_csv(targeted_mz_list_path)\n",
    "\n",
    "# Step 2: Initialize an Empty DataFrame to Store Results\n",
    "combined_results = pd.DataFrame()\n",
    "\n",
    "# Step 3: Iterate Over Each Targeted m/z\n",
    "for idx, row in targeted_mz_list.iterrows():\n",
    "    target_mz = row['m/z']\n",
    "    concentrations = [row['conc1'], row['conc2'], row['conc3'], row['conc4'], row['conc5']]\n",
    "\n",
    "    # Step 4: Find m/z Matches Within 10 ppm\n",
    "    ppm_tolerance = 50\n",
    "    matching_rows = processed_data.copy()\n",
    "    matching_rows['ppm'] = abs((matching_rows['Average_MZ'] - target_mz) / target_mz) * 1e6\n",
    "    matching_rows = matching_rows[matching_rows['ppm'] <= ppm_tolerance]\n",
    "\n",
    "    # Step 5: Add Metadata from Targeted m/z List to Matching Rows\n",
    "    for col in ['group_id', 'DTXSID', 'compound_id', 'SMILES', 'MOLECULAR_FORMULA', 'm/z']:\n",
    "        matching_rows[col] = row[col]\n",
    "\n",
    "    # Step 6: Calculate LogRF for Each Treatment/Concentration Pair with CV Filtering\n",
    "    logRF_columns = []\n",
    "    for i, (treatment, conc) in enumerate(zip([\"100x\", \"200x\", \"1000x\", \"5000x\", \"25000x\"], concentrations), start=1):\n",
    "        avg_intensity_col = f\"avg_intensity_{treatment}\"\n",
    "        cv_col = f\"cv_percent_{treatment}\"\n",
    "        logRF_col = f'LogRF_{i}'\n",
    "        if avg_intensity_col in matching_rows.columns and cv_col in matching_rows.columns and conc != 0:\n",
    "            matching_rows[logRF_col] = np.where(matching_rows[cv_col] <= 30, np.log10(matching_rows[avg_intensity_col] / conc), np.nan)\n",
    "        else:\n",
    "            matching_rows[logRF_col] = np.nan\n",
    "        logRF_columns.append(logRF_col)\n",
    "\n",
    "    # Step 7: Calculate Average LogRF if More Than 3 Values are Available\n",
    "    matching_rows['average_LogRF'] = matching_rows[logRF_columns].apply(lambda x: x[x.notna()].mean() if x.count() >= 3 else np.nan, axis=1)\n",
    "\n",
    "    # Step 8: Combine Results\n",
    "    combined_results = pd.concat([combined_results, matching_rows], ignore_index=True)\n",
    "\n",
    "# Step 9: Filter Combined Results to Keep Only Rows with Valid average_LogRF and Lowest ppm\n",
    "filtered_results = combined_results[combined_results['average_LogRF'].notna()]\n",
    "filtered_results = filtered_results.sort_values(by=['m/z', 'ppm']).drop_duplicates(subset='m/z', keep='first')\n",
    "\n",
    "# Step 10: Export Combined Results\n",
    "output_folder = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/targeted_res/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "combined_results_path = os.path.join(output_folder, \"combined_targeted_mz_results.csv\")\n",
    "filtered_results.to_csv(combined_results_path, index=False)\n",
    "print(f\"Combined targeted m/z analysis results saved: {combined_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress, pearsonr\n",
    "\n",
    "# Step 1: Load the Cleaned Data\n",
    "# Load the cleaned LC-MS data file\n",
    "path1 = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/499_500_501_area_filtered.csv'\n",
    "filtered_data = pd.read_csv(path1)\n",
    "\n",
    "# Step 2: Separate Data by Groups\n",
    "# Extract the data for each group separately\n",
    "unique_groups = filtered_data.columns.str.extract(r'(E\\d{3})')[0].dropna().unique()\n",
    "print(f\"Unique groups found: {unique_groups}\")\n",
    "\n",
    "# Step 3: Iterate over each group\n",
    "for group in unique_groups:\n",
    "    # Extract group-specific data, keep 'Average_MZ' and 'RT' columns\n",
    "    group_data = filtered_data.filter(like=group)\n",
    "    group_data = pd.concat([filtered_data[['Average_MZ', 'RT']], group_data], axis=1)\n",
    "    print(f\"Processing group: {group}, Columns extracted: {group_data.columns.tolist()}\")\n",
    "\n",
    "    # Step 4: Data Cleaning for the Group\n",
    "    # Remove any columns or rows with a large number of missing values, or impute missing values\n",
    "    group_data_clean = group_data.dropna(axis=0, thresh=len(group_data.columns) * 0.5)\n",
    "    group_data_clean = group_data_clean.dropna(axis=1, thresh=len(group_data_clean) * 0.5)\n",
    "    group_data_clean.fillna(group_data_clean.mean(), inplace=True)\n",
    "    print(f\"Group data cleaned: {group_data_clean.shape}\")\n",
    "\n",
    "    # Step 5: Calculate CV% for Each Treatment and Add to Data\n",
    "    for treatment in [\"100x\", \"200x\", \"1000x\", \"5000x\", \"25000x\"]:\n",
    "        treatment_cols = [col for col in group_data_clean.columns if f\"_{treatment}_\" in col]\n",
    "        if len(treatment_cols) == 0:\n",
    "            group_data_clean[f\"{treatment}_cv\"] = np.nan\n",
    "            continue\n",
    "        treatment_data = group_data_clean[treatment_cols]\n",
    "        cv_percent = treatment_data.std(axis=1) / treatment_data.mean(axis=1) * 100\n",
    "        group_data_clean[f\"{treatment}_cv\"] = cv_percent\n",
    "    print(f\"Group data with CV% added: {group_data_clean.head()}\")\n",
    "\n",
    "    # Step 6: Filter Data Based on CV% Threshold\n",
    "    cv_columns = [col for col in group_data_clean.columns if col.endswith('_cv')]\n",
    "    filtered_group_data = group_data_clean[(group_data_clean[cv_columns] <= 30).sum(axis=1) >= 3]\n",
    "    print(f\"Filtered group data (CV% <= 30 for at least 3 treatments): {filtered_group_data.shape}\")\n",
    "\n",
    "    # Step 7: Export Filtered Data for the Group\n",
    "    output_folder = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/plot_res/'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    filtered_data_path = os.path.join(output_folder, f\"filtered_feature_table_{group}.csv\")\n",
    "    filtered_group_data.to_csv(filtered_data_path, index=False)\n",
    "    print(f\"Filtered data saved for group {group}: {filtered_data_path}\")\n",
    "\n",
    "# Step 8: Targeted m/z Search and Analysis\n",
    "# Load the targeted m/z list\n",
    "mz_list_path = 'D:/UCSF_postdoc_topic/ESI_/ENTACT/first_ENTACT_mixtures_mzlist.csv'\n",
    "targeted_mz_list = pd.read_csv(mz_list_path)\n",
    "print(f\"Targeted m/z list loaded: {len(targeted_mz_list)} entries\")\n",
    "\n",
    "# Initialize an empty list to store results for the group\n",
    "results = []\n",
    "\n",
    "# Iterate over each targeted m/z\n",
    "for idx, row in targeted_mz_list.iterrows():\n",
    "    target_mz = row['m/z']\n",
    "    compound_id = row['compound_id']\n",
    "    group_id = row['group_id']\n",
    "    concentrations = [row['conc1'], row['conc2'], row['conc3'], row['conc4'], row['conc5']]\n",
    "\n",
    "    # Only process if the group_id matches the current group\n",
    "    if group_id not in unique_groups:\n",
    "        continue\n",
    "\n",
    "    group_data_clean = pd.read_csv(os.path.join(output_folder, f\"filtered_feature_table_{group_id}.csv\"))\n",
    "\n",
    "    # Find m/z matches within 10 ppm\n",
    "    ppm_tolerance = 20\n",
    "    matches = group_data_clean['Average_MZ'].apply(lambda mz: abs((mz - target_mz) / target_mz) * 1e6 <= ppm_tolerance)\n",
    "    matching_mzs = group_data_clean[matches]\n",
    "    print(f\"Target m/z: {target_mz}, Matches found: {len(matching_mzs)}\")\n",
    "\n",
    "    # Rename m/z if multiple matches are found\n",
    "    if len(matching_mzs) > 1:\n",
    "        matching_mzs['Average_MZ'] = [f\"{mz}_{i+1}\" for i, mz in enumerate(matching_mzs['Average_MZ'])]\n",
    "\n",
    "    # Calculate average intensity and CV% for each treatment\n",
    "    avg_intensities = []\n",
    "    valid_concentrations = []\n",
    "    cv_percents = []\n",
    "    for treatment, conc in zip([\"100x\", \"200x\", \"1000x\", \"5000x\", \"25000x\"], concentrations):\n",
    "        treatment_cols = [col for col in matching_mzs.columns if f\"_{treatment}_\" in col]\n",
    "        if len(treatment_cols) == 0:\n",
    "            avg_intensities.append(np.nan)\n",
    "            valid_concentrations.append(conc)\n",
    "            cv_percents.append(np.nan)\n",
    "            continue\n",
    "        treatment_data = matching_mzs[treatment_cols]\n",
    "        avg_intensity = treatment_data.mean(axis=1).mean()  # Averaging the intensity across the treatment columns\n",
    "        cv_percent = (treatment_data.std(axis=1) / treatment_data.mean(axis=1) * 100).mean()  # Average CV%\n",
    "        avg_intensities.append(avg_intensity)\n",
    "        valid_concentrations.append(conc)\n",
    "        cv_percents.append(cv_percent)\n",
    "    print(f\"Average intensities: {avg_intensities}, CV%: {cv_percents}, Valid concentrations: {valid_concentrations}\")\n",
    "\n",
    "    # Remove NaN values for correlation analysis\n",
    "    filtered_avg_intensities = [val for val in avg_intensities if not np.isnan(val)]\n",
    "    filtered_concentrations = [conc for val, conc in zip(avg_intensities, valid_concentrations) if not np.isnan(val)]\n",
    "    filtered_cv_percents = [cv for cv in cv_percents if not np.isnan(cv)]\n",
    "\n",
    "    # Check if there are enough valid treatments to proceed\n",
    "    if len(filtered_avg_intensities) < 3:\n",
    "        print(f\"Not enough valid treatments for target m/z {target_mz}\")\n",
    "        continue\n",
    "\n",
    "    # Count treatments with CV% < 30\n",
    "    filtered_concentrations = [conc for conc, cv in zip(filtered_concentrations, filtered_cv_percents) if cv < 30]\n",
    "    filtered_avg_intensities = [val for val, cv in zip(filtered_avg_intensities, filtered_cv_percents) if cv < 30]\n",
    "    if len(filtered_avg_intensities) < 3:\n",
    "        print(f\"Not enough treatments with CV% < 30 for target m/z {target_mz}\")\n",
    "        continue\n",
    "\n",
    "    # Perform correlation analysis\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(filtered_concentrations, filtered_avg_intensities)\n",
    "    print(f\"Correlation analysis for target m/z {target_mz}: Slope: {slope}, R^2: {r_value**2}\")\n",
    "\n",
    "    # Plot scatter plot and regression line\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(filtered_concentrations, filtered_avg_intensities, label='Data Points')\n",
    "    plt.plot(filtered_concentrations, intercept + slope * np.array(filtered_concentrations), 'r', label=f'Fit Line (R^2={r_value**2:.2f})')\n",
    "    plt.xlabel('Concentration')\n",
    "    plt.ylabel('Average Intensity')\n",
    "    plt.title(f'Scatter Plot and Linear Regression for m/z {target_mz} in Group {group_id}')\n",
    "    plt.legend()\n",
    "    plot_path = os.path.join(output_folder, f\"scatter_plot_{group_id}_{target_mz}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved for target m/z {target_mz}: {plot_path}\")\n",
    "\n",
    "    # Save results\n",
    "    results.append({\n",
    "        \"m/z\": target_mz,\n",
    "        \"compound_id\": compound_id,\n",
    "        \"group_id\": group_id,\n",
    "        \"avg_intensities\": avg_intensities,\n",
    "        \"cv_percents\": cv_percents,\n",
    "        \"cv_below_30_count\": len(filtered_avg_intensities),\n",
    "        \"pearson_r2\": r_value**2,\n",
    "        \"slope\": slope,\n",
    "        \"intercept\": intercept\n",
    "    })\n",
    "\n",
    "# Step 9: Save Results to DataFrame for All Groups\n",
    "results_df = pd.DataFrame(results)\n",
    "results_path = os.path.join(output_folder, \"targeted_mz_results_all_groups.csv\")\n",
    "if not results_df.empty:\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"Results saved for all groups: {results_path}\")\n",
    "else:\n",
    "    print(f\"No results to save for any group.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  Total_Targeted_m/z  Number_of_Hits\n",
      "0   499                  95              75\n",
      "1   500                  94              70\n",
      "2   501                  95              71\n",
      "3   502                  95              75\n",
      "4   504                 185             148\n",
      "5   506                 365             275\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to match m/z values from targeted list with Average_MZ column and filter based on ppm tolerance\n",
    "# do MS1 search, find the # of detected compounds\n",
    "def match_mz(targeted_mz_list, data, ppm_threshold=20):\n",
    "    results = []\n",
    "\n",
    "    for targeted_mz in targeted_mz_list:\n",
    "        # Calculate ppm difference for each value in Average_MZ column\n",
    "        data['ppm_diff'] = abs(data['Average_MZ'] - targeted_mz) / targeted_mz * 1e6\n",
    "\n",
    "        # Filter rows where ppm difference is within the threshold\n",
    "        filtered_hits = data[data['ppm_diff'] <= ppm_threshold]\n",
    "\n",
    "        # Append the number of hits for the targeted m/z value\n",
    "        results.append({'Targeted_MZ': targeted_mz, 'Number_of_Hits': len(filtered_hits)})\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Batch processing for multiple files\n",
    "batches = ['499', '500', '501', '502', '504', '506']\n",
    "output_results = {}\n",
    "summary_results = []\n",
    "\n",
    "\n",
    "for batch in batches:\n",
    "    # Determine the correct folder for the targeted m/z list based on the batch number\n",
    "    if batch in ['499', '500', '501']:\n",
    "        targeted_mz_list_folder = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_499_500_501/peak_area_blank_filtered/'\n",
    "    else:\n",
    "        targeted_mz_list_folder = 'D:/UCSF_postdoc_topic/ESI_/DTSC_DATA_ENTACT/ENTACT_502_504_506/peak_area_with_blank_filter/'\n",
    "    \n",
    "    # Load processed data and targeted m/z list for each batch\n",
    "    processed_data_path = os.path.join(input_folder, f'filtered_feature_table_pos_EPA{batch}.csv')\n",
    "    targeted_mz_list_path = os.path.join(targeted_mz_list_folder, f'{batch}_batch_target_mzlist.csv')\n",
    "    \n",
    "    processed_data = pd.read_csv(processed_data_path)\n",
    "    targeted_mz_list = pd.read_csv(targeted_mz_list_path)\n",
    "\n",
    "    # Match m/z values and get the number of hits\n",
    "    results_df = match_mz(targeted_mz_list['m/z'], processed_data, 30)\n",
    "    num_hits = results_df[results_df['Number_of_Hits'] > 0].shape[0]\n",
    "    \n",
    "    output_results[batch] = results_df\n",
    "    summary_results.append({'Batch': batch, 'Total_Targeted_m/z': len(targeted_mz_list), 'Number_of_Hits': num_hits})\n",
    "\n",
    "# Create a summary table of the results\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(summary_df)\n",
    "\n",
    "# Example output access\n",
    "# To access results for a specific batch, use: output_results['499']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
